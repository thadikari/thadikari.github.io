<!--
  Tharindu Adikari, University of Moratuwa
  Tharindu B. Adikari, University of Toronto
  thadikari
  -->

I am currently pursuing a Ph.D. degree in electrical and computer engineering at University of Toronto under the supervision of Prof. [Stark C. Draper](https://www.ece.utoronto.ca/people/draper-s). I received the B.Sc. degree from the University of Moratuwa, Sri Lanka, in 2014, and the M.A.Sc. degree from the University of Toronto, Canada, in 2018. From 2014 to 2016, I worked with [LSEG Technology](https://www.lseg.com/resources/perspectives-global-markets/capital-market-technology/millenniumit-delivering-speed-stability-and-success) (formerly known as MillenniumIT), Sri Lanka, primarily on stock market surveillance systems. 

My research interests include distributed optimization, machine learning and source coding. [This](https://www.linkedin.com/in/thadikari) is my Linkedin page.
I have tabulated below a few projects I've worked on. Visit [Github](https://www.github.com/thadikari) to see all projects.


|||
|-|-|
| <br><br> ___Two-Terminal Source Coding With Common Sum Reconstruction___ <br> [[_Paper_](https://ieeexplore.ieee.org/search/searchresult.jsp?newsearch=true&queryText=Two-Terminal%20Source%20Coding%20With%20Common%20Sum%20Reconstruction)], _Published_: ISIT, 2022 <br><br> An information theoretic analysis of the two-terminal source coding with Common Sum Reconstruction (CSR) problem. Each terminal wants to reconstruct the sum of two correlated sources with the constraint that their reconstructions must be identical with high probability. <br><br><br> | <img src="https://raw.githubusercontent.com/thadikari/thadikari.github.io/master/adikari2022two-terminal_banner.png" width="900" /> 
| <br><br> ___Compressing gradients by exploiting temporal correlation in momentum-SGD___ <br> [[_Paper_](https://ieeexplore.ieee.org/document/9511618), [_Arxiv_](https://arxiv.org/abs/2108.07827)], _Published_: JSAIT, 2021 <br><br> A predictive coding based approach for exploiting memory in consecutive momentum-SGD updates vectors. A novel predictor design is proposed for systems with error-feedback. <br><br><br> | <img src="https://raw.githubusercontent.com/thadikari/thadikari.github.io/master/adikari2021compressing_banner.png" width="900" /> 
| <br><br> ___Asynchronous Delayed Optimization With Time-Varying Minibatches___ <br> [[_Paper_](https://ieeexplore.ieee.org/document/9429693), [_Code_](https://github.com/thadikari/anytime_minibatch)], _Published_: JSAIT, 2021 <br><br> TensorFlow-based implementation of SGD on a master-worker system with time-varying minibatch sizes (when workers complete different amounts of work due to stragglers). <br><br><br> | <img src="https://raw.githubusercontent.com/thadikari/anytime_minibatch/master/data/cifar10_all_plots.png" width="900" /> 
| <br><br> ___Decentralized Optimization with Non-Identical Sampling in Presence of Stragglers___ <br> [[_Paper_](https://ieeexplore.ieee.org/document/9053329), [_Arxiv_](https://arxiv.org/abs/2108.11071), [_Code_](https://github.com/thadikari/graph_optimization)], _Published_: ICASSP, 2020 <br><br> An analysis on decentralized consensus optimization (e.g. optimization on a graph) when workers sample data from non-identical distributions and perform variable amounts of work due to stragglers (slow nodes). <br><br><br> | <img src="https://raw.githubusercontent.com/thadikari/graph_optimization/master/data/archive10_icassp_final_results/run_fashion_mnist_linear1_distinct_PWG_perfect_amb_iclr_10_bern_08_60_10_metro.png?raw=true" width="900"/> <br> <img src="https://raw.githubusercontent.com/thadikari/graph_optimization/master/data/archive10_icassp_final_results/run_fashion_mnist_linear1_distinct_PWG_rand_walk_amb_iclr_10_bern_08_60_10_metro.png?raw=true" width="900"/> 
| <br><br> ___Efficient Learning of Neighbor Representations for Boundary Trees and Forests___ <br> [[_Paper_](https://ieeexplore.ieee.org/document/8693043), [_Arxiv_](https://arxiv.org/abs/1810.11165), [_Code_](https://github.com/thadikari/boundary-training)], _Published_: CISS, 2019 <br><br> This project builds on the "Differentiable Boundary Trees" algorithm by Zoran et. al. proposed in [_this_](https://arxiv.org/pdf/1702.08833.pdf) paper. I have posted [_here_](https://github.com/thadikari/differentiable-boundary-trees) one possible implementation of the algorithm. <br><br><br> | <img src="https://raw.githubusercontent.com/thadikari/differentiable-boundary-trees/master/results/nn_tsne_6k.png?raw=true" width="900" /> 
| <br><br> ___Python/NumPy implementation of a few iterative decoders for LDPC codes___ <br> [[_Code_](https://github.com/thadikari/ldpc_decoders)] <br><br> Includes implementations of min-sum and sum-product algorithms using sparse matrices (`scipy.sparse`), maximum-likedlood (ML) and linear-programming (LP) decoders, and ADMM decoder. <br><br><br> | <img src="https://raw.githubusercontent.com/thadikari/decoders/master/data/plots/BSC_SPA_compare.png?raw=true" width="900"/> |



<!---
<table>
  
  <tr>
    <td>
      A TensorFlow-based implementation of the differentiable boundary trees algorithm.
      <br/><br/>
      Paper: https://arxiv.org/pdf/1702.08833.pdf
      <br/>
      Source: https://github.com/thadikari/differentiable-boundary-trees
      <br/><br/><br/>
      Simulation results in efficient learning of neighbor representations for boundary trees and forests
      <br/><br/>
      Paper: arxiv.org/abs/1810.11165 - CISS, 2019
      <br/>
      Source: https://github.com/thadikari/boundary-training
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/thadikari/differentiable-boundary-trees/master/results/nn_tsne_6k.png?raw=true" width="600" />
    </td>
  </tr>

  <tr>
    <td>
      Python/NumPy implementation of a few iterative decoders for LDPC codes.
      <br/><br/>
      Source: https://github.com/thadikari/ldpc_decoders
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/thadikari/decoders/master/data/plots/BSC_SPA_compare.png?raw=true" width="600"/>
    </td>
  </tr>
 </table>
-->
